{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seonils/.conda/envs/dl/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.init # for initialization\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import torchvision.utils as utils\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import dataset as d\n",
    "import util as u\n",
    "from pathlib import Path\n",
    "\n",
    "import h5py # making dataset (or db) file maybe\n",
    "import pickle # ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ImageFolder dataset\n",
    "#all jpgs need to be wrapped by the directory of having the same number\n",
    "vist_train=d.ImgDset_Folder(Path(\"./data/train\"))  \n",
    "vist_val=d.ImgDset_Folder(Path(\"./data/val\"))  \n",
    "vist_test=d.ImgDset_Folder(Path(\"./data/test\"))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['2.png'], PosixPath('data/train'), 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vist_train.img_names, vist_train.img_path, len(vist_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size=1 \n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(dataset=vist_train,\n",
    "                                          batch_size=1,                 #no batch needed\n",
    "                                          shuffle=False,                #no need to shuffle: feature extraction\n",
    "                                          num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /home/seonils/.torch/models/resnet50-19c8e357.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "#model here\n",
    "Resnet=models.resnet50(pretrained=True) #pretrained on \"ImageNet\", \n",
    "                                        #expects identically normalized/shaped inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resnet.cuda() #no graphic driver for desktop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_dataset=len(vist_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torchvision.datasets.folder' has no attribute 'pil_loader'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2cd793612101>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdsets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvist_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvist_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torchvision.datasets.folder' has no attribute 'pil_loader'"
     ]
    }
   ],
   "source": [
    "#tt=transforms.ToTensor\n",
    "#for i in range(10):\n",
    "#    a=tt.__call__(tt,pic=dsets.folder.pil_loader(vist_train.imgs[i][0]))[2][50][10:20]\n",
    "#    b=tt.__call__(tt,vist_train[i][0])[2][50][10:20]\n",
    "#    print(a==b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,img enumerate(vist_train):\n",
    "    #X=Variable(img).cuda()\n",
    "    X=Variable(img)\n",
    "    feature=Resnet(x)\n",
    "    img_name_list=vist_train.img_names\n",
    "    print(\"\\n\\tfilename: {original} is read \".format(original=img_name_list[idx]))\n",
    "    print(\"\\tfeature shape: {fshape}\".format(fshape=feature.shape))\n",
    "    namebase=img_name_list[idx].split(\".\")[0] # extension stripped out\n",
    "    h5py.open(namebase+\".\", \"w\")\n",
    "    print(\"\\tsaved as {hdf5}\".format )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dl]",
   "language": "python",
   "name": "conda-env-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
